import os
import sys
import pytz
import datetime
import torch
import torch.nn as nn
import torch.optim as optim
import random
from stable_baselines3 import DQN
from stable_baselines3.common.callbacks import BaseCallback
from sumo_rl import SumoEnvironment 
# Make sure you have the sumo_rl package installed

# Function to get current datetime
def get_datetime():
    utc_now = pytz.utc.localize(datetime.datetime.utcnow())
    currentDT = utc_now.astimezone(pytz.timezone("America/Los_Angeles"))
    DATIME = currentDT.strftime("%Y-%m-%d %H:%M:%S")
    return DATIME

# PyTorch Model
class TrafficAutoEncoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(TrafficAutoEncoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size, input_size),
            nn.ReLU()
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

# PyTorch Dataset
class TrafficDataset(Dataset):
    def __init__(self, data, max_sample_size):
        self.data = data
        self.max_sample_size = max_sample_size

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        tensor_sample = [float(elem) if isinstance(elem, (int, float)) else 0.0 for elem in sample[1:]]
        # Pad with zeros to match the maximum size
        if len(tensor_sample) < self.max_sample_size:
            tensor_sample.extend([0.0] * (self.max_sample_size - len(tensor_sample)))
        return torch.tensor(tensor_sample, dtype=torch.float32)

# Custom Callback for Logging Rewards
class RewardLoggerCallback(BaseCallback):
    def _on_step(self) -> bool:
        print("Callback executed!")  # Debug print
        if "infos" in self.locals.keys():
            print(self.locals.get("infos"))  # Print entire 'infos' structure
            rewards = [info.get('reward') for info in self.locals.get("infos") if 'reward' in info]
            for reward in rewards:
                print(f"Reward: {reward}")
        return True

def setup_sumo_home():
    if "SUMO_HOME" in os.environ:
        tools = os.path.join(os.environ["SUMO_HOME"], "tools")
        sys.path.append(tools)
    else:
        sys.exit("Please declare the environment variable 'SUMO_HOME'")

def main():
    print("Hello")
    setup_sumo_home()

    env = SumoEnvironment(
        net_file="nets/single-intersection/single-intersection.net.xml",
        route_file="nets/single-intersection/single-intersection.rou.xml",
        out_csv_name="outputs/single-intersection/double_ql/d",
        single_agent=True,
        use_gui=True,
        num_seconds=40000,
    )

    print("Environment setup done.")

    # Initialize the DQN model
    model = DQN(
        env=env,
        policy="MlpPolicy",
        learning_rate=0.001,
        learning_starts=0,
        train_freq=1,
        target_update_interval=500,
        exploration_initial_eps=0.05,
        exploration_final_eps=0.01,
        verbose=0  # Suppress verbose output
    )

    num_episodes = 10
    timesteps_per_episode = env.num_seconds if hasattr(env, "num_seconds") else 1000
    total_timesteps = num_episodes * timesteps_per_episode

    # Train the model
    model.learn(total_timesteps=total_timesteps)

    # Save the trained model (uncomment this if you want to save the model)
    # model.save("saved_models/single-intersection/dqn/max_60")

if __name__ == "__main__":
    main()
